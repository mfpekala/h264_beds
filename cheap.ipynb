{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cheap Experimentation\n",
    "\n",
    "This notebook contains scratch work for visualizing the equivariant activations and quantifying the embedding error when using this \"Cheap\" approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Optional\n",
    "from small_gg import SmallGGAutoencoder\n",
    "from codec import Chunker, custom_load_img\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show(chunk):\n",
    "    \"\"\"\n",
    "    Given a 2-dimensional greyscale rep of an image, show it\n",
    "    \"\"\"\n",
    "    plt.imshow(chunk, cmap=\"gray\", vmin=0.0, vmax=1.0)\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "def show2(chunk1, chunk2):\n",
    "    \"\"\"\n",
    "    Given two 2-dimensional grayscale representations of images, show them side by side.\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(1, 2)\n",
    "    axes[0].imshow(chunk1.squeeze(), cmap=\"gray\", vmin=0.0, vmax=1.0)\n",
    "    axes[0].axis(\"off\")\n",
    "    axes[1].imshow(chunk2.squeeze(), cmap=\"gray\", vmin=0.0, vmax=1.0)\n",
    "    axes[1].axis(\"off\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = custom_load_img(\"frames/basic/i/frame_0.png\")\n",
    "chunks = [chunk for chunk in Chunker(img).to_chunks()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_simple_test(chunk, shift):\n",
    "    \"\"\"\n",
    "    Given a chunk (16x16 ndarray) and shift (tuple[int, int]) produce a\n",
    "    48x48 ndarray with the 16x16 in the center, offset by shift\n",
    "    \"\"\"\n",
    "    result = np.zeros((48, 48), dtype=chunk.dtype)\n",
    "    result[16 + shift[1] : 16 + shift[1] + 16, 16 + shift[0] : 16 + shift[0] + 16] = chunk\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shift = (1, 1)\n",
    "chunk = random.choice(chunks)\n",
    "test = generate_simple_test(chunk.detach().numpy().reshape((16, 16)), shift)\n",
    "post_chunk = test[16 : 32, 16 : 32]\n",
    "post_chunk = torch.from_numpy(post_chunk).unsqueeze(0)\n",
    "print(\"Visualizing test case...\")\n",
    "show2(chunk.detach().numpy().reshape((16, 16)), post_chunk)\n",
    "\n",
    "model = SmallGGAutoencoder.load_model(\"autoencoders/gen3_200.pt\")\n",
    "og_prelude, og_bed = model.embed_with_prelude(chunk)\n",
    "correct_prelude, correct_bed = model.embed_with_prelude(post_chunk)\n",
    "print(\"\")\n",
    "print(\"Visualizing actual prelude difference...\")\n",
    "show2(og_prelude.detach().numpy(), correct_prelude.detach().numpy())\n",
    "\n",
    "prelude_size = 16\n",
    "naive_prelude = np.zeros((prelude_size, prelude_size), dtype=np.float32)\n",
    "relevant = og_prelude.detach().numpy().reshape((prelude_size, prelude_size))\n",
    "naive_prelude[shift[1] : prelude_size, shift[0] : prelude_size] = relevant[0 : prelude_size - shift[1], 0 : prelude_size - shift[0]]\n",
    "print(correct_prelude[0,0,0])\n",
    "\n",
    "print(\"Showing naive difference\")\n",
    "correct_clean = correct_prelude.detach().numpy().reshape((prelude_size, prelude_size))\n",
    "show(np.abs(naive_prelude - correct_clean))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = custom_load_img(\"frames/basic/i/frame_0.png\")\n",
    "chunks = [chunk for chunk in Chunker(img, custom_size=32).to_chunks()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shift_chunk(chunk, shift):\n",
    "    \"\"\"\n",
    "    Shift a chunk with out-of-bounds handling\n",
    "    \"\"\"\n",
    "    shifted_chunk = np.zeros_like(chunk)\n",
    "    \n",
    "    # Determine the ranges for copying from the original chunk to the shifted chunk\n",
    "    row_range = slice(max(0, -shift[0]), min(chunk.shape[0], chunk.shape[0] - shift[0]))\n",
    "    col_range = slice(max(0, -shift[1]), min(chunk.shape[1], chunk.shape[1] - shift[1]))\n",
    "    \n",
    "    # Determine the ranges for pasting into the shifted chunk\n",
    "    new_row_range = slice(max(0, shift[0]), min(chunk.shape[0], chunk.shape[0] + shift[0]))\n",
    "    new_col_range = slice(max(0, shift[1]), min(chunk.shape[1], chunk.shape[1] + shift[1]))\n",
    "    \n",
    "    # Perform the shift and handle out-of-bounds\n",
    "    shifted_chunk[new_row_range, new_col_range] = chunk[row_range, col_range]\n",
    "    \n",
    "    return shifted_chunk\n",
    "\n",
    "def simulate_diff(chunk, frac_random=0.25):\n",
    "    \"\"\"\n",
    "    Slightly too lazy to actually get the chunks from consecutive frames so\n",
    "    just going to add in some randomness\n",
    "    Will replace `frac_random` of the entries with 1/64 * random number in [0, 63]\n",
    "    NOTE: Does NOT modify chunk returns a new chunk\n",
    "    \"\"\"\n",
    "    num_set = int((chunk.shape[0] * chunk.shape[1]) * frac_random)\n",
    "    result = chunk.copy()\n",
    "    for _ in range(num_set):\n",
    "        row = random.randint(0, chunk.shape[0] - 1)\n",
    "        col = random.randint(0, chunk.shape[1] - 1)\n",
    "        mult = sum([random.randint(-1, 1) for _ in range(3)])\n",
    "        new_val = result[row, col] + 1.0 / 64.0 * mult\n",
    "        result[row, col] = max(min(new_val, 1.0), 0.0)\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ix = random.randint(0, len(chunks) - 1)\n",
    "ix = 65\n",
    "chunk_tens = chunks[ix]\n",
    "chunk_np = chunk_tens.detach().numpy().reshape((32, 32))\n",
    "chunk = chunks[ix].detach().numpy().reshape((32, 32))\n",
    "shift = (-2, 3)\n",
    "shifted = shift_chunk(chunk, shift)\n",
    "diff = simulate_diff(shifted)\n",
    "print(\"Will be running test on the following chunks\")\n",
    "show2(chunk, diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def box_show(chunk, boxes=None, offset=(-0.5, -0.5)):\n",
    "    \"\"\"\n",
    "    Given a 2-dimensional greyscale rep of an image, show it\n",
    "    \"\"\"\n",
    "    plt.imshow(chunk, cmap=\"gray\", vmin=0.0, vmax=1.0)\n",
    "    \n",
    "    if boxes:\n",
    "        for box, color in boxes:\n",
    "            top_left, bottom_right = box\n",
    "            # Apply offset to coordinates\n",
    "            top_left = (top_left[0] + offset[0], top_left[1] + offset[1])\n",
    "            bottom_right = (bottom_right[0] + offset[0], bottom_right[1] + offset[1])\n",
    "            # Plot the box\n",
    "            plt.plot([top_left[0], bottom_right[0], bottom_right[0], top_left[0], top_left[0]],\n",
    "                     [top_left[1], top_left[1], bottom_right[1], bottom_right[1], top_left[1]], color=color, linewidth=4)\n",
    "    \n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "box_show(chunk, [(((16, 0), (32, 16)),\"red\"), (((13, 2), (29, 18)),\"pink\")])\n",
    "box_show(diff, [(((16, 0), (32, 16)),\"pink\"), ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_size = 16\n",
    "import math\n",
    "\n",
    "def dx_to_top_left(dx):\n",
    "    img_height = 32\n",
    "    chunk_height = int(math.ceil(img_height / chunk_size))\n",
    "    x = (dx // chunk_height) * chunk_size\n",
    "    y = (dx % chunk_height) * chunk_size\n",
    "    return (x, y)\n",
    "\n",
    "og_preludes = np.zeros_like(chunk)\n",
    "correct_preludes = np.zeros_like(shifted)\n",
    "\n",
    "for dx, c in enumerate(Chunker(chunk).to_chunks()):\n",
    "    og_prelude, _ = model.embed_with_prelude(c)\n",
    "    x, y = dx_to_top_left(dx)\n",
    "    og_preludes[y : y + chunk_size, x : x + chunk_size] = og_prelude.detach().numpy()\n",
    "\n",
    "for dx, c in enumerate(Chunker(shifted).to_chunks()):\n",
    "    shifted_prelude, _ = model.embed_with_prelude(c)\n",
    "    x, y = dx_to_top_left(dx)\n",
    "    correct_preludes[y : y + chunk_size, x : x + chunk_size] = shifted_prelude.detach().numpy()\n",
    "\n",
    "box_show(og_preludes, [(((16, 0), (32, 16)),\"red\"), (((13, 2), (29, 18)),\"pink\")])\n",
    "box_show(correct_preludes, [(((16, 0), (32, 16)),\"pink\"), ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "guessed = og_preludes[2 : 18, 13 : 29]\n",
    "correct = correct_prelude[0 : 16, 16 : 32]\n",
    "acc_diff = guessed - correct\n",
    "show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "apm220",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
